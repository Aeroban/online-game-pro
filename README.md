# online-game-pro

<h2> Introduction </h2>

Game developers and publishers often check the reviews of their game to get information whether the game they published are doing great or not. Thus, they need data of the game reviews to be stored so it can be analyzed and answer the following questions:

1. Average rating for each game
2. Average feedback for games made by X developer
3. Total players that gave feedback in a specific month for each game

However, the data used in this project is limited as the web scrap code is not the best there is and can still be improved. The tools used are Flask + SQLAlchemy, MySQL, BeautifulSoup, Bootstrap, and Visual Paradigm for the diagram.

<h2> Data Model </h2>

![Data Model](/static/data_model.jpg)

The data model is based on snowflake schema because it require less space and has no redundant data. Additionally, since there are only 7 tables in the schema, there should be no problem regarding the complexity.

The dimension tables are used to store context for the fact table. Using the snowflake schema, the dimension tables are:

game_DIM that has one developer in developer_DIM and can have many categories by connecting it to the category_DIM using game_category_BR bridge table. Each game has to at least connected to one category.
category_DIM is connected to game_DIM via BR table. One category has to be at least be a child to one game.
developer_DIM contains information of the game developer that can create multiple games. Since the developer id was not attached in the game page, the id in here is generated by db to act as a fake natural key (replacement for real developer id natural key).
date_DIM that contains date information for each day.
user_DIM that contains information regarding the user that gives the reviews. The user id natural key cannot also be found in the review page, thus it will be replaced by id generated by the code. The generated id is intended to act like real natural key.

The game_DIM table is still categorized as dimensional table even though it contains date because the date is treated as a metadata for when the game was released.

The only fact table in the model is review_fact. The table contains keys to the corresponding game, user, and date the review was posted, the rating (0 = not recommended, 1 = recommended), the review content (comments), and current column (0 = old review, 1 = current review) as player can update their review to be different for each day.

time_created and time_updated are intended for ETL purpose such as the last time the table was updated. Those columns' types are written as timestamp even though it should be datetime because there was no datetime datatype in Visual Paradigm. The key are referring to the surrogate key generated by GUID class (https://gist.github.com/gmolveau/7caeeefe637679005a7bb9ae1b5e421e credit: gmolveau)

<h2> Web Scrap </h2>
The data for this project is scrapped from steam (https://store.steampowered.com/) using BeautifulSoup. The pages taken are the game pages and the review community hub page for each game. For the game page, the taken data are the game itself, categories, and the developer. For the review community hub page, the data taken are the review itself (only the first review page with most recent filter) and the user data. Thus, the limitation are:

* Generating a lot of review data is difficult. The scrap command has to run several times to get a lot of reviews.
* Developer id and user id cannot be obtained using the current code as it does not include a crawler that can go to the developer and user link to get the id's. Yes, both information are not available in the game page and the review community hub page.

Nevertheless, the data model can still serve its purpose.

<h2> How it works </h2>
<h4> Initialize environment </h4>
As the project require a lot of package, a virtual environment is recommended to be used in the project. To use virtual environment, first we have to install it by typing `pip install virtualenv`. After the virtual environment has been installed, we can implement it to the project by typing `virtualenv env` in the cmd. A new folder named env will show up.

To activate virtual env we have first to make sure the execution policy is unrestricted. So, we can type in the cmd `Set-ExecutionPolicy Unrestricted -Scope Process`. Then, we can start the virtual env by `./env/Scripts/activate.ps1` (windows). In the env terminal, we install the required packages by using `pip install -r requirements.txt`.

<h4> Initialize database </h4>
The database is first initialized. It can be done by creating the db with name "online_game" in MySQL. Next, we access the python terminal by typing `py` in the cmd. Then we type `from app import db` and then `db.create_all()`. The database schema should have been created in MySQL.

<h4> Time table </h4>
First the time table data is imported from .csv file by using pandas and insert it into MySQL database. The insertion is done per row. The current date used in this project (28 October 2021) only contains October 2021 dates only. However, it is possible to increase the date range by importing a .csv table that has more time range. The .csv can be imported by using the init_date(.csv file path) function in the app.py file.

<h4> Game and review </h4>
For the game and review tables, it can be inserted using creating GameScrapper object called game_scrapper and add_game() function in the app.py file. A steam game page link then can be inserted into the command line. The game scrapper then scrap the steam game and the game community hub review pages to retrieve the game, developer, category, user, and the review information.

![Add game and review](/static/game_scrap.png)

![Input](/static/input.png)

The retrieved data are then validated to see if the row already exists by using the key. And then a model will be created and inserted into the database. For the review data, another validation will be done before inserting the data to check whether another review on the same game has been written before by the same user. If yes, change the current status of the older reviews into 0 (not current review) and add the newest review with current 1 (current review). This will enable the data analyst to see whether players change their mind after a certain time (before - after update) or not. However, please note that since the date_key is based on a single day, it means there could only be one review for one day and the current system does not support overwrite for the review (same game_key and player_key) on the same date. It will only apply append row if the review is from different day.

<h4> Running the website </h4>
After initializing and adding data into the database, make sure to add app.run(debug=True) command in the app.py file. The file then can be run by typing py app.py in the command line.

<h4> Example files </h4>
Example files such as the sql data and the date fill can be found inside the project files (April and May 2022). It can be used as testing purpose.

<h4> Screen shots </h4>
<img src="https://user-images.githubusercontent.com/75673604/169320413-c9926559-84db-46e9-a581-eeaff5cc233d.png">
![1](https://user-images.githubusercontent.com/75673604/169320413-c9926559-84db-46e9-a581-eeaff5cc233d.png)
![2](https://user-images.githubusercontent.com/75673604/169320432-ab8d2a89-470d-4328-892c-1dfbbe26ff19.png)

<h2> Linked in link:</h2>
www.linkedin.com/in/gabriella-ryanie-setiawan-5893b3224
